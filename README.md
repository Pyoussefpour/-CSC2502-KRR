# CSC2502 â€“ Knowledge Representation and Reasoning (Graduate Course)

This repository contains coursework and a paper presentation for **CSC2502: Knowledge Representation and Reasoning**, a graduate-level course in the Department of Computer Science at the **University of Toronto**.

## ğŸ“˜ Course Overview

CSC2502 introduces the formal foundations and practical tools for representing knowledge and reasoning about it. The course explores logic-based knowledge representation systems and techniques used in artificial intelligence, automated planning, and semantic reasoning.

Key topics include:
- Propositional and First-Order Logic
- Default and Non-monotonic Reasoning
- Description Logics and Ontologies
- Autoepistemic and Circumscription Logics
- Resolution and Proof Strategies
- Knowledge Representation for Planning

---

## ğŸ“ Assignment Summaries

### ğŸ§  Assignment 1 â€“ Formal Logic and Reasoning
- Translated natural language puzzles into formal first-order logic (FOL).
- Constructed and proved logical equivalences and entailments using Horn clauses.
- Applied resolution to derive conclusions in geometry and equivalence-based reasoning.

### ğŸ§© Assignment 2 â€“ Non-monotonic Reasoning
- Explored the limits of default logic, circumscription, and autoepistemic logic.
- Formalized defaults and exceptions using abnormality predicates.
- Compared weak vs. strong default representations and analyzed extensions under various logics.

### ğŸ§® Assignment 3 â€“ Description Logics and Action Formalisms
- Designed TBoxes and ABoxes to model ontologies involving vehicles, parts, and agents.
- Reasoned about satisfiability and subsumption using DL axioms.
- Modeled planning problems using STRIPS operators, progression, and regression planning.
- Represented domain dynamics in both logical and STRIPS-based formalism.

---

## ğŸ“„ Paper Presentation

### ğŸ“š *Continual Reasoning: Non-monotonic Reasoning in Neurosymbolic AI using Continual Learning*
- Presented a paper from the NeSy 2023 workshop.
- Explored Logic Tensor Networks (LTNs) and continual learning strategies to improve reasoning under evolving knowledge bases.
- Discussed the challenges of non-monotonic reasoning and how sequential task learning can mitigate issues like catastrophic forgetting.
- Evaluated on symbolic reasoning tasks such as the Penguin Exception and Smokers-Friends-Cancer models.

---

## âš ï¸ Academic Integrity

This repository is intended for educational and reference purposes only. Please do not copy or submit this work if you are currently enrolled in the course.

